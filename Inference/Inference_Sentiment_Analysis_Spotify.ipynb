{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Lib and Dep"
      ],
      "metadata": {
        "id": "E9h6eSBMnkKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk  # Import NLTK (Natural Language Toolkit).\n",
        "nltk.download('punkt_tab')  # Dataset for tokenization\n",
        "nltk.download('stopwords')  # Dataset for stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize  # Tokenize text\n",
        "from nltk.corpus import stopwords  # Stopwords list\n",
        "\n",
        "import re, string\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMLxMwizekWk",
        "outputId": "27ab633c-7b1c-4ddc-9942-1da4f76fa882"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model and Dataset"
      ],
      "metadata": {
        "id": "LvOU-AhAFSHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UnGVHsKFG3A",
        "outputId": "b1383311-036a-4860-9a0c-6148b4a317af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('app_review.csv')\n",
        "df_limit = df.sample(n=50000, random_state=42)\n",
        "\n",
        "# Load trained model\n",
        "model_lstm = load_model('model_lstm.h5')\n",
        "model_gru = load_model('model_gru.h5')\n",
        "model_rnn = load_model('model_simplernn.h5')\n",
        "\n",
        "# Label mapping\n",
        "labels = ['negative', 'neutral', 'positive']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "JGJdPM8UQQkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Special Characters"
      ],
      "metadata": {
        "id": "ypcxzG6hQdYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # delete mention\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # delete hashtag\n",
        "    text = re.sub(r'RT[\\s]', '', text) # delete RT (for tweets only)\n",
        "    text = re.sub(r\"http\\S+\", '', text) # delete link\n",
        "    text = re.sub(r'[0-9]+', '', text) # delete number\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # delete other special characters\n",
        "\n",
        "    text = text.replace('\\n', ' ') # replace new row with space\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # replace punctuation\n",
        "    text = text.strip(' ') # delete space from left and right side of the text\n",
        "    return text"
      ],
      "metadata": {
        "id": "KqN7ItxjQgK8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Folding"
      ],
      "metadata": {
        "id": "lkCGt0TgQSrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def casefoldingText(text): # Lower the text\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "2nilDiwgQgxM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "uJr0wqWSRoDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizingText(text): # Tokenize the text\n",
        "    text = word_tokenize(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "13BlkNf-Rp5m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords & Slangwords"
      ],
      "metadata": {
        "id": "zmy1_VT5R0tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filteringText(text): # delete stopwords\n",
        "    listStopwords = set(stopwords.words('english'))\n",
        "\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:\n",
        "            filtered.append(txt)\n",
        "    text = filtered\n",
        "    return text"
      ],
      "metadata": {
        "id": "Hf9nYJvnR14e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slangwords = {}\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()\n",
        "    fixed_words = []\n",
        "\n",
        "    for word in words:\n",
        "        if word.lower() in slangwords:\n",
        "            fixed_words.append(slangwords[word.lower()])\n",
        "        else:\n",
        "            fixed_words.append(word)\n",
        "\n",
        "    fixed_text = ' '.join(fixed_words)\n",
        "    return fixed_text"
      ],
      "metadata": {
        "id": "togPXDRrSU-W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "5pOcTlZCTGCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatizingText(text): # Reduce the word to its base\n",
        "    # Create lemmatizer object\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    words = text\n",
        "\n",
        "    # Implement lemmatizing for each word\n",
        "    lemma_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "\n",
        "    # Join the lemmatized word\n",
        "    lemmatized_text = ' '.join(lemma_words)\n",
        "\n",
        "    return lemmatized_text"
      ],
      "metadata": {
        "id": "HWUdw4uVTHzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47915dae-97cc-4656-de5e-bfac323e14ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying"
      ],
      "metadata": {
        "id": "S9vokCIqSYLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessingText(text):\n",
        "  # clean the text and save it to text_clean\n",
        "  clean_text = cleaningText(text)\n",
        "\n",
        "  # Lower the text and save it to text_casefoldingText\n",
        "  casefolding_text = casefoldingText(clean_text)\n",
        "\n",
        "  # Tokenize the text and save it to text_tokenizingText\n",
        "  tokenize_text = tokenizingText(casefolding_text)\n",
        "\n",
        "  # Delete stopwords and save it to text_stopword\n",
        "  stopword_text = filteringText(tokenize_text)\n",
        "\n",
        "  # reduce the word to its base word\n",
        "  lemmatized_text = lemmatizingText(stopword_text)\n",
        "\n",
        "  return lemmatized_text"
      ],
      "metadata": {
        "id": "bIXJClB0S_94"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Setup"
      ],
      "metadata": {
        "id": "WQi-4DlPUG9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Setup\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pickle\n",
        "\n",
        "preprocessed_texts = df_limit['content'].apply(preprocessingText)  # pastikan list of string\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(preprocessed_texts)\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "5dygr4Hih8UE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def textToSequence(text):\n",
        "  # preprocessing the text\n",
        "  preprocessed_text = preprocessingText(text)\n",
        "\n",
        "  # print(preprocessed_text)\n",
        "\n",
        "  # Use the same tokenizer for the same tokenize index\n",
        "  with open('tokenizer.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "  # Konversi ke urutan angka\n",
        "  sequences = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "\n",
        "  # print(sequences)\n",
        "\n",
        "  # padding to standardize the length\n",
        "  maxlen = 100\n",
        "  padded_text = pad_sequences(sequences, padding='post', maxlen=maxlen)\n",
        "\n",
        "  return padded_text\n",
        "\n",
        "def predictLstm(text):\n",
        "  # convert text to sequence\n",
        "  padded_text = textToSequence(text)\n",
        "\n",
        "  # prediction\n",
        "  pred = model_lstm.predict(padded_text)\n",
        "\n",
        "  predicted_class = np.argmax(pred, axis=1)[0]\n",
        "  confidence = np.max(pred)\n",
        "\n",
        "  return labels[predicted_class], float(confidence)\n",
        "\n",
        "def predictGru(padded_text):\n",
        "  # convert text to sequence\n",
        "  padded_text = textToSequence(text)\n",
        "\n",
        "  # prediction\n",
        "  pred = model_gru.predict(padded_text)\n",
        "\n",
        "  predicted_class = np.argmax(pred, axis=1)[0]\n",
        "  confidence = np.max(pred)\n",
        "\n",
        "  return labels[predicted_class], float(confidence)\n",
        "\n",
        "def predictRnn(padded_text):\n",
        "  # convert text to sequence\n",
        "  padded_text = textToSequence(text)\n",
        "\n",
        "  # prediction\n",
        "  pred = model_rnn.predict(padded_text)\n",
        "\n",
        "  predicted_class = np.argmax(pred, axis=1)[0]\n",
        "  confidence = np.max(pred)\n",
        "\n",
        "  return labels[predicted_class], float(confidence)"
      ],
      "metadata": {
        "id": "jn5WeZiMaOz9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "fG0EIxJznz7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive\n",
        "text = \"This app is perfect!\"\n",
        "sent_lstm, conf_lstm = predictLstm(text)\n",
        "print(f\"Prediksi LSTM: {sent_lstm} (Confidence LSTM: {conf_lstm:.2f})\")\n",
        "sent_gru, conf_gru = predictGru(text)\n",
        "print(f\"Prediksi GRU: {sent_gru} (Confidence LSTM: {conf_gru:.2f})\")\n",
        "sent_rnn, conf_rnn = predictRnn(text)\n",
        "print(f\"Prediksi RNN: {sent_rnn} (Confidence LSTM: {conf_rnn:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d53Ve3cYcquc",
        "outputId": "1eeb4784-f7ea-4f66-a583-7a8a86411f69"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Prediksi LSTM: positive (Confidence LSTM: 1.00)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "Prediksi GRU: positive (Confidence LSTM: 1.00)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Prediksi RNN: positive (Confidence LSTM: 1.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative\n",
        "text = \"This app is really bad!\"\n",
        "sent_lstm, conf_lstm = predictLstm(text)\n",
        "print(f\"Prediksi LSTM: {sent_lstm} (Confidence LSTM: {conf_lstm:.2f})\")\n",
        "sent_gru, conf_gru = predictGru(text)\n",
        "print(f\"Prediksi GRU: {sent_gru} (Confidence LSTM: {conf_gru:.2f})\")\n",
        "sent_rnn, conf_rnn = predictRnn(text)\n",
        "print(f\"Prediksi RNN: {sent_rnn} (Confidence LSTM: {conf_rnn:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsyev7MHm-cN",
        "outputId": "54cda389-1d54-4585-84b0-c16ed2efc1fb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Prediksi LSTM: negative (Confidence LSTM: 1.00)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Prediksi GRU: negative (Confidence LSTM: 1.00)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Prediksi RNN: negative (Confidence LSTM: 0.54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neutral\n",
        "text = \"This app allows you to listen to music all day!\"\n",
        "sent_lstm, conf_lstm = predictLstm(text)\n",
        "print(f\"Prediksi LSTM: {sent_lstm} (Confidence LSTM: {conf_lstm:.2f})\")\n",
        "sent_gru, conf_gru = predictGru(text)\n",
        "print(f\"Prediksi GRU: {sent_gru} (Confidence LSTM: {conf_gru:.2f})\")\n",
        "sent_rnn, conf_rnn = predictRnn(text)\n",
        "print(f\"Prediksi RNN: {sent_rnn} (Confidence LSTM: {conf_rnn:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGmme2ybnMlF",
        "outputId": "7ab9eb3e-38e3-471c-fb49-01f0bae2af83"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Prediksi LSTM: neutral (Confidence LSTM: 0.77)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Prediksi GRU: neutral (Confidence LSTM: 0.96)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "Prediksi RNN: positive (Confidence LSTM: 0.72)\n"
          ]
        }
      ]
    }
  ]
}